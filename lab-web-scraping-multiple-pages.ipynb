{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c5cb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b521c",
   "metadata": {},
   "source": [
    "# 1. MVP (Minimum Viable Product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877d20e",
   "metadata": {},
   "source": [
    "### 1.1 Hot List from Listchallenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43784eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.listchallenges.com/the-top-1000-billboard-hot-100-songs-of-all-time\"\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8c36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4a027",
   "metadata": {},
   "source": [
    "> The page has 25 pages, so we have to iterate the urls to add all the data to the soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4098b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, 25)\n",
    "\n",
    "pages = []\n",
    "\n",
    "for i in iterations:\n",
    "    \n",
    "    start_at= str(i)\n",
    "    url = \"https://www.listchallenges.com/the-top-1000-billboard-hot-100-songs-of-all-time/list/\" + start_at\n",
    "    response = requests.get(url, headers = {\"Accept-Language\": \"en-US\"})\n",
    "    pages.append(response)\n",
    "\n",
    "    wait_time = randint(1,4000)\n",
    "    sleep(wait_time/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d0a60",
   "metadata": {},
   "source": [
    "> Now that we have a variable with the pages, we can iterate each page and extract the information of every page.\n",
    "> Each title-artist is separated (items are not in a table). But all of them follow the same pattern, so we can access them with an iterable string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39e24aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "\n",
    "for i in range(len(pages)):\n",
    "    soup = BeautifulSoup(pages[i].content, \"html.parser\")\n",
    "\n",
    "    for j in range(1,46):\n",
    "        number=j\n",
    "        str_number=str(number)\n",
    "        string = '#repeaterListItems > div:nth-child(' +str_number+ ') > div > div.item-name'\n",
    "        selection=soup.select(string)\n",
    "        list1.append(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e520315",
   "metadata": {},
   "source": [
    "> The items of list1 are bs4.element.ResultSet, and the item of the item (list1[0][0]), is bs4.element.Tag. BeautifulSoup attributes like item.text, don´t work with resultset, so we need to apply it to the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82e21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "list2=[]\n",
    "\n",
    "for k in range(0,1125):\n",
    "    for item in list1[k]:\n",
    "        list2.append(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0fac1",
   "metadata": {},
   "source": [
    "> List2 items have this format we need to clean: '\\r\\n                                        The Twist - Chubby Checker\\r\\n                                    '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5fe5372",
   "metadata": {},
   "outputs": [],
   "source": [
    "list3 = [item.strip() for item in list2[0:1000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969e0b5",
   "metadata": {},
   "source": [
    "> Finally, we only need to split the song and the artist, and put it on a df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08912923",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = []\n",
    "artists = []\n",
    "for element in list3:\n",
    "    parts = element.split(' - ')\n",
    "    if len(parts) == 2:\n",
    "        song = parts[0].strip()\n",
    "        artist = parts[1].strip()\n",
    "        songs.append(song)\n",
    "        artists.append(artist)\n",
    "            \n",
    "df_list_cha = pd.DataFrame({'song':songs, 'artist':artists})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4f5fc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Twist</td>\n",
       "      <td>Chubby Checker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smooth</td>\n",
       "      <td>Santana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MacK the Knife</td>\n",
       "      <td>Bobby Darin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leann Rimes</td>\n",
       "      <td>How Do I Live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Party Rock Anthem</td>\n",
       "      <td>LMFAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>Wooly Bully</td>\n",
       "      <td>Sam the Sham and the Pharoahs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>Joyride</td>\n",
       "      <td>Roxette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>Kansas City</td>\n",
       "      <td>Wilbert Harrison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>Pumped Up Kicks</td>\n",
       "      <td>Foster the People</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>Lips of an Angel</td>\n",
       "      <td>Hinder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>935 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  song                         artist\n",
       "0            The Twist                 Chubby Checker\n",
       "1               Smooth                        Santana\n",
       "2       MacK the Knife                    Bobby Darin\n",
       "3          Leann Rimes                  How Do I Live\n",
       "4    Party Rock Anthem                          LMFAO\n",
       "..                 ...                            ...\n",
       "930        Wooly Bully  Sam the Sham and the Pharoahs\n",
       "931            Joyride                        Roxette\n",
       "932        Kansas City               Wilbert Harrison\n",
       "933    Pumped Up Kicks              Foster the People\n",
       "934   Lips of an Angel                         Hinder\n",
       "\n",
       "[935 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list_cha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5556d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_cha.to_csv('hot.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6058c20",
   "metadata": {},
   "source": [
    "> I can´t extract all of them because some have this pattern: 'Mark Ronson, \"Uptown Funk\" (Feat. Bruno Mars)'. I´ll do when I have more time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740eca79",
   "metadata": {},
   "source": [
    "# 2  Practice web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ea43c",
   "metadata": {},
   "source": [
    "### 2.1  Retrieve an arbitrary Wikipedia page of \"Python\" and create a list of links on that page: \n",
    "\n",
    "url ='https://en.wikipedia.org/wiki/Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be2d60dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://en.wikipedia.org/wiki/Python'\n",
    "\n",
    "response=requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5aad26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d25e895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "for i in range(len(soup.select(\"a\"))):\n",
    "    list1.append(soup.select(\"a\")[i]['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30c0be",
   "metadata": {},
   "source": [
    "> The items of list1 are strings. We only want those that start by \"/wiki/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2fdc2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = []\n",
    "\n",
    "for i in list1:\n",
    "    links = re.findall(r'/wiki/[\\w]+', i)\n",
    "    list2.extend(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bfbc5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/Main_Page',\n",
       " '/wiki/Wikipedia',\n",
       " '/wiki/Portal',\n",
       " '/wiki/Special',\n",
       " '/wiki/Wikipedia',\n",
       " '/wiki/Wikipedia',\n",
       " '/wiki/Special',\n",
       " '/wiki/Help',\n",
       " '/wiki/Help',\n",
       " '/wiki/Wikipedia',\n",
       " '/wiki/Special',\n",
       " '/wiki/Wikipedia',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Special',\n",
       " '/wiki/Help',\n",
       " '/wiki/Special',\n",
       " '/wiki/Special',\n",
       " '/wiki/Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Pitono_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Pitono',\n",
       " '/wiki/Python',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Mboma_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Special',\n",
       " '/wiki/Python',\n",
       " '/wiki/Talk',\n",
       " '/wiki/Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Special',\n",
       " '/wiki/Special',\n",
       " '/wiki/Wikipedia',\n",
       " '/wiki/Special',\n",
       " '/wiki/Special',\n",
       " '/wiki/Category',\n",
       " '/wiki/Python',\n",
       " '/wiki/python',\n",
       " '/wiki/Pythonidae',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/CMU_Common_Lisp',\n",
       " '/wiki/PERQ',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Colt_Python',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_',\n",
       " '/wiki/Timon_of_Phlius',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/Pithon',\n",
       " '/wiki/File',\n",
       " '/wiki/Help',\n",
       " '/wiki/Help',\n",
       " '/wiki/Category',\n",
       " '/wiki/Category',\n",
       " '/wiki/Category',\n",
       " '/wiki/Category',\n",
       " '/wiki/Category',\n",
       " '/wiki/Category',\n",
       " '/wiki/Category',\n",
       " '/wiki/Wikipedia',\n",
       " '/wiki/Wikipedia',\n",
       " '/wiki/Terms_of_Use',\n",
       " '/wiki/Privacy_policy',\n",
       " '/wiki/Special',\n",
       " '/wiki/Wikipedia',\n",
       " '/wiki/Wikipedia',\n",
       " '/wiki/Wikipedia',\n",
       " '/wiki/Special',\n",
       " '/wiki/Special']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd63320d",
   "metadata": {},
   "source": [
    "### 2.2  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe: \n",
    "\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ade7a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://www.emsc-csem.org/Earthquake/'\n",
    "\n",
    "response=requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfce0377",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637da957",
   "metadata": {},
   "source": [
    "> I´ve tried to get the results, and I also tried to get the id to navigate to the webpage information. But I couldn´t manage to access it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068ca2d",
   "metadata": {},
   "source": [
    "### 2.3  Create a Python list with the top ten FBI's Most Wanted names: \n",
    "\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "676ab3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://www.fbi.gov/wanted/topten'\n",
    "\n",
    "response=requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7805b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b0fbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "for i in range(1,11):\n",
    "    number=i\n",
    "    str_number=str(number)\n",
    "    string = '#query-results-0f737222c5054a81a120bce207b0446a > ul > li:nth-child(' +str_number+ ') > h3 > a'\n",
    "    selection=soup.select(string)\n",
    "    list1.append(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fc121b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list2=[]\n",
    "\n",
    "for k in range(0,10):\n",
    "    for item in list1[k]:\n",
    "        list2.append(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a8da4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WILVER VILLEGAS-PALOMINO',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'RUJA IGNATOVA',\n",
       " 'DONALD EUGENE FIELDS II',\n",
       " 'ALEXIS FLORES',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'OMAR ALEXANDER CARDENAS',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
